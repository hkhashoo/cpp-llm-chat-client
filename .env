# Provider (ollama | openai | hf | mock)
PROVIDER=ollama

# Ollama server address
OLLAMA_HOST=http://localhost:11434

# Model to use (must be pulled with `ollama pull <model>`)
MODEL=llama3

# Stream responses as tokens? (true|false)
STREAM=false

# Max turns to keep in chat history
HISTORY_LIMIT=5

# HTTP request timeout (ms)
TIMEOUT_MS=15000
